<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1" name="viewport" >
    <meta content="This is a technical demo from the Model Registry team of Open Data Hub. Content: - model-registry-operator - Notebook → Model Registry - DS Pipeline → Model Registry - Model Registry → Serving - ..." name="description"> 
    <title>Model Registry tech demo 20231121</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css" rel="stylesheet">
    <style>
.blog-header {
  font-family: "Playfair Display", Georgia, "Times New Roman", serif/*rtl:Amiri, Georgia, "Times New Roman", serif*/;
}
    </style>
</head>
<body>	
<header>
  <div class="navbar navbar-dark bg-dark shadow-lg">
    <div class="container">
      <a href="https://matteomortari.com" class="navbar-brand d-flex align-items-center">
        <strong>matteomortari.com</strong>
      </a>
    </div>
  </div>
</header>

<main><section class="my-2 py-5 container">
	<div class="page-header">
		<h1 class="blog-header">Model Registry tech demo 20231121</h1>
	</div>

	<p><em>21 November 2023</em></p>

	<div class="row justify-content-center text-center py-3">
 <div class="col-lg-6">
  <div class="ratio ratio-16x9">
   <iframe src="https://www.youtube.com/embed/grXnjGtDFXg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </div>
 </div>
</div>
<p>This is a <a href="https://en.wikipedia.org/wiki/Technology_demonstration">technical demo</a> from the <a href="https://github.com/opendatahub-io/model-registry">Model Registry</a> team of <a href="https://opendatahub.io">Open Data Hub</a>.</p>
<p>Content:<br />
- model-registry-operator<br />
- Notebook → Model Registry<br />
- DS Pipeline → Model Registry<br />
- Model Registry → Serving<br />
- One More Thing...<br />
- Conclusions</p>
<p>This post does not go into details of context and background of the Model Registry, for which we invite you to checkout <a href="https://docs.google.com/document/d/1T3KfOqIfJohp0s1koQ2XrJJQQhj7TECO-m2xPsW59_c/edit?usp=sharing">this document</a> instead.</p>
<h2>model-registry-operator</h2>
<p>We can use the <a href="https://github.com/opendatahub-io/model-registry-operator">Model Registry Operator</a> to install the Model Registry in our Kubernetes platform.<br />
Installing the operator is easy, following a few steps as detailed in the README.</p>
<p>First, we install a Custom Resource Definition for our Model Registry:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2020.54.21%20(2).png" alt="" /></p>
<p>Then, we deploy the <code>model-registry-operator</code> which at present sits in its own namespace:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2020.58.10%20(2).png" alt="" /></p>
<p>Finally, we create a Model Registry instance:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.01.57%20(2).png" alt="" /></p>
<p>With an instance of Model Registry now available, we can move to perform some ML training using Notebook in order to later index the resulting models on this model registry instance.</p>
<h2>Notebook → Model Registry</h2>
<p>For the scope of this technical demo of Model Registry, we will be training and making inference with the <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST dataset</a>, serving as an &quot;hello world&quot; style of example for neural network usages.</p>
<p>We are given some training data set to perform training: given the following image, the training label is defined as '3':</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.14.11%20(2).png" alt="" /></p>
<p>So, given the following image from the test data set, we would like our ML model (once trained) to successfully predict the label '4':</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.15.26%20(2).png" alt="" /></p>
<h3>Notebook → Model Registry: First model version</h3>
<p>We can start training a first version of our neural network:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.17.03%20(2).png" alt="" /></p>
<p>And after some quick dry-run to check the prediction seems reasonable:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.17.32%20(2).png" alt="" /></p>
<p>We can store the trained ML model in an S3-compatible bucket:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.19.46%20(2).png" alt="" /></p>
<p>And index the model in our Model Registry:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.19.53%20(2).png" alt="" /></p>
<h3>Notebook → Model Registry: Second model version</h3>
<p>For the scope of this technical demo, we can now train a second version of our ML model, by defining instead a convolutional neural network:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.22.50%20(2).png" alt="" /></p>
<p>We can perform again some quick dry-run to check the prediction seems reasonable, store the trained model in the S3 bucket, and index the model in the Model Registry:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.24.43%20(2).png" alt="" /></p>
<p>We can notice in the outputs the entities created on the Model Registry.</p>
<h3>Notebook → Model Registry: REST APIs</h3>
<p>We don't have yet a GUI web application to display the metadata in our Model Registry, but we can perform this exercise by making use of the REST APIs from the command line.</p>
<p>We can display the known RegisteredModel, in this case we called our RegisteredModel <code>MNIST</code> (as we've been using the MNIST dataset):</p>
<pre><code>MR_HOSTNAME=...

curl --silent -X 'GET' \
  &quot;$MR_HOSTNAME/api/model_registry/v1alpha1/registered_models?pageSize=100&amp;orderBy=ID&amp;sortOrder=DESC&amp;nextPageToken=&quot; \
  -H 'accept: application/json' | jq .items
</code></pre>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.29.58%20(2).png" alt="" /></p>
<p>We can display the two versions we have just indexed, which are referencing as well the models as Stored in S3:</p>
<pre><code>curl --silent -X 'GET' \
  &quot;$MR_HOSTNAME/api/model_registry/v1alpha1/model_versions?pageSize=100&amp;orderBy=ID&amp;sortOrder=DESC&amp;nextPageToken=&quot; \
  -H 'accept: application/json' | jq .items
</code></pre>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.32.42%20(2).png" alt="" /></p>
<p>And we can display details for the ModelArtifact corresponding to the latter version:</p>
<pre><code>curl --silent -X 'GET' \
  &quot;$MR_HOSTNAME/api/model_registry/v1alpha1/model_versions/4/artifacts&quot; \
  -H 'accept: application/json' | jq .items
</code></pre>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.33.59%20(2).png" alt="" /></p>
<p>We can move now to Data Science Pipeline.</p>
<h2>DS Pipeline → Model Registry</h2>
<p>We can create a Data Science Pipeline (DSP) which performs analogous steps in training, validating, storing and indexing the ML model:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.40.37%20(2).png" alt="" /></p>
<p>We can create a DSP Run, possibly using specific parameters which influence the model training and validation; since the trained model matches the requirements defined in the pipeline, this run successfully completed, the model is stored once again on a S3-compatible bucket and the metadata are indexed on the Model Registry. We have now 3 versions of our ML model:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2021.52.50%20(2).png" alt="" /></p>
<p>We have now a bunch of of ModelVersion(s) available on the Model Registry, ready to be deployed for inference using Model Serving.</p>
<h2>Model Registry → Serving</h2>
<p>We can use the following REST API to create an entity for InferenceService on the Model Registry:</p>
<pre><code>curl -X 'POST' \
  $MR_HOSTNAME'/api/model_registry/v1alpha1/serving_environments/1/inference_services' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  &quot;name&quot;: &quot;mnist-e2e&quot;,
  &quot;modelVersionId&quot;: &quot;4&quot;,
  &quot;runtime&quot;: &quot;mmserver1&quot;,
  &quot;registeredModelId&quot;: &quot;2&quot;,
  &quot;servingEnvironmentId&quot;: &quot;1&quot;,
  &quot;state&quot;: &quot;DEPLOYED&quot;
}'
</code></pre>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2022.05.03%20(2).png" alt="" /></p>
<p>It's important to note the Model Registry is <em>not</em> an active orchestrator; what actually happens behind the scene is that the K8s Model Controller performs a reconciliation loop and based on the metadata we just created on the Model Registry, create a CR for ModelMesh in order to deploy our ML for Inference:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2022.09.01%20(2).png" alt="" /></p>
<p>We can as well undeploy the InferenceService by changing flipping the status on the Model Registry; in this case the same reconcilation loop will trigger again, but will proceed accordingly to remove the ModelMesh CR for Serving:</p>
<pre><code>curl -X 'PATCH' \
  &quot;$MR_HOSTNAME/api/model_registry/v1alpha1/inference_services/6&quot; \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -d '{
  &quot;modelVersionId&quot;: &quot;4&quot;,
  &quot;runtime&quot;: &quot;mmserver1&quot;,
  &quot;state&quot;: &quot;UNDEPLOYED&quot;
}'
</code></pre>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2022.13.00%20(2).png" alt="" /></p>
<h2>One More Thing...</h2>
<p>To wrap-up this technical exercise, we can re-deploy once more our trained ML model and noting the inference service endpoint:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2022.16.26%20(2).png" alt="" /></p>
<p>As we can use it to deploy some Intelligent Application on Kubernetes:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2022.21.41%20(2).png" alt="" /></p>
<p>Which in this case is a (very raw cut!) PoC application exercising our ML inference endpoint on ModelMesh for Model Serving; as we draw by hand:</p>
<p><img src="model-registry-tech-demo-20231121/Screenshot%202023-12-02%20at%2022.23.00%20(2).png" alt="" /></p>
<p>We will get back an array of probabilities; since index 2 is the most probable, our intelligent application is predicting we have drawn the <code>number: 2</code> thanks to the model we just trained, indexed and deployed, all while making use of Model Registry!</p>
<h2>Conclusions</h2>
<p>In this short technical demo we have explored some of the capabilites of the Model Registry by making an end-to-end demo.</p>
<p>We have installed the model-registry-operator in order to deploy our instance of Model Registry.</p>
<p>We have performed a couple of training of ML neural networks, stored them in an S3 bucket and indexed those model in the Model Registry. We have also performed analogously from the perspective of Data Science Pipelines, which may include parametrization, model validation, etc.</p>
<p>We have seen how Model Registry is <em>not</em> an active orchestrator but a metadata repository; following established Kubernetes best-practices a reconciliation loop from the Model Controller make sure to enact the correct CustomResource deployment and update in the cluster. This mechanism is used to govern deployment of ML models which the Model Registry have previously maintained index, per the previous steps.</p>
<p>Finally, we have seen how to leverage the Inference endpoint of the deployed ML model to drive the end-to-end workload of intelligent application on top of Kubernetes!</p>
<p>If you found this content interesting, don't hesitate to join the conversation:<br />
- <a href="https://github.com/kubeflow/kubeflow/issues/7396">https://github.com/kubeflow/kubeflow/issues/7396</a><br />
- <a href="https://www.kubeflow.org/docs/about/community/#kubeflow-community-call">https://www.kubeflow.org/docs/about/community/#kubeflow-community-call</a></p>


</section>
</main>

<footer class="text-muted py-5">
  <div class="container">
    <p class="float-end mb-1">
      <a href="#">Back to top</a>
    </p>
    <p>&copy; Matteo Mortari. All rights reserved
      <br/><a href="https://www.linkedin.com/in/matteomortari" class="text-reset">LinkedIn</a>
      <br/><a href="https://www.youtube.com/MatteoMortari" class="text-reset">YouTube</a>
      <br/><a href="https://github.com/tarilabs" class="text-reset">Github</a>
      <!-- <br/><a href="https://twitter.com/tari_manga" class="text-reset">Twitter</a> -->
    </p>
  </div>
</footer>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-gtEjrD/SeCtmISkJkNUaaKMoLD0//ElJ19smozuHV6z3Iehds+3Ulb9Bn9Plx0x4" crossorigin="anonymous"></script>

  </body>
</html>